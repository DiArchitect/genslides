import nltk.data
import re
from os import listdir
from os.path import isfile, join
from transformers import GPT2Tokenizer
import math

tokenizer = nltk.data.load('nltk:tokenizers/punkt/english.pickle')

result = tokenizer.tokenize('Hello. This is a test. It works!')

print(result)

print(result[1])

print(result[1] == 'This is a test.')
print(result[1] == 'This is not a test.')
if "This is a test." in result:
    print('Yes')

if "This is a not test." in result:
    print('Yes2')

mypath = "../saved/"

onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]

print(onlyfiles)


text = "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    OpenAI Developer Forum\n  \n\n\n\n\n\nUrgent Assistance Required - Excessive Response Time with OpenAI API Integration\n\n\n\n\n\n\nAPI\n\n\n\n\n\n\n\ngpt-35-turbo, \n            api-speed, \n            api\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nadmin34\n\n\n\n\n                    May 16, 2023,  8:04am\n                  \n\n1\n\n\n\nDear Developers,\nI am writing to request assistance with an issue that we are currently experiencing with the OpenAI API (GPT 3.5 Turbo) integrated into our PHP based web application. Our web app uses Dialogflow ES to power a conversational chatbot, and we have connected Dialogflow ES with OpenAI, adding a knowledge module for enhanced functionality using a webhook.\nHere is the code:\n//ini_set('display_errors', 1);\n//ini_set('display_startup_errors', 1);\n//error_reporting(E_ALL);\n\nheader('Content-Type: application/json');\nrequire('../config.php');\ninclude 'vendor/autoload.php';\nuse Orhanerday\\OpenAi\\OpenAi;\n\n$data = json_decode(file_get_contents('php://input'), true);\n$query = $data['queryResult']['queryText'];\n//$query = \"what is SPOT\";\n\nif($query == \"sb-follow-up-form\"){\n    return;\n}\n\n$url = \"knowledge.txt\";\n$content = file_get_contents($url);\n\n$messages_array = [];\n$messages_array[] = ['role' => 'system', 'content' => 'Following is information about our business, You have to act like you are the a human agent in our support system and do not let the user know about the fact you are an AI language model. This is information of my business:' . $content. 'If the user says or ask anything which is irrelevant with spot, Deny that request or dont respond to that statement by saying that you cannot help them with anything out of SPOT, make sure to not blow your cover by saying you are an AI model. If the user asks anything about you, you are supposed to introduce yourself as \"Agent for SPOT Support\", If they ask you how are you or anything regarding your or their feelings, make sure to never state that you are an AI Lanugage model.'];\n$support_board_data = $data['originalDetectIntentRequest']['payload']['support_board'];\n$conversation_id = $support_board_data['conversation_id'];\n$user_id = $support_board_data['user_id'];\n\n//$conversation_id = 293;\n//$user_id = 334;\n\n$is_last_automated = false;\n\nif($conversation_id){\n    $conversation_data = sb_get_conversation($user_id, $conversation_id);\n    foreach ($conversation_data['messages'] as $message) {\n        $role = ($message['user_type'] === 'agent' || $message['user_type'] === 'bot') ? 'assistant' : 'user';\n        if (strpos($message['message'], '[email id=') === false ) {\n            if (!empty($message['payload'])) {\n                $payload = json_decode($message['payload'], true);\n                if (isset($payload['rich-messages']['sb-follow-up-form']['result'])) {\n                    $result = $payload['rich-messages']['sb-follow-up-form']['result'];\n                    $first_name = $result['first_name'][0] ?? '';\n                    $last_name = $result['last_name'][0] ?? '';\n                    $email = $result['email'][0] ?? '';\n                    $phone = $result['phone'][0] ?? '';\n                    $content = \"Follwing is my information: First Name: $first_name, Last Name: $last_name, Email: $email, Phone: $phone\";\n                    $messages_array[] = ['role' => \"user\", 'content' => $content];\n                    $is_last_automated = true;\n                } else {\n                    $messages_array[] = ['role' => $role, 'content' => $message['message']];\n                    $is_last_automated = false;\n                }\n            }else{\n                $messages_array[] = ['role' => $role, 'content' => $message['message']];\n                $is_last_automated = false;\n            }\n        }\n    }\n}\n$messages_array[] = ['role' => 'user', 'content' => $query];\n\nif($query && $is_last_automated == false)\n{   \n    $open_ai = new OpenAi(\"\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0H7cCwDI8\");\n    $gpt_response = call_gpt($open_ai, $messages_array);\n    //$gpt_response = \"I cant answer that :D\";\n    $dialogflow_response = format_dialogflow_response($gpt_response);\n\n    echo json_encode($dialogflow_response);\n}\n\n\n\nfunction call_gpt($open_ai, $messages_array) {\n\n    \n    $headers = [\n        'Content-Type: application/json',\n        'Authorization: Bearer '\n    ];\n\n\n    $chat = $open_ai->chat([\n        'model' => 'gpt-3.5-turbo',\n        'messages' => $messages_array,\n        'max_tokens' => 150,\n        'temperature' => 0.5,\n        'top_p' => 0.8,\n    ]);\n\n    $response_data = json_decode($chat, true);\n    $gpt_response = $response_data['choices'][0]['message']['content'];\n\n    return $gpt_response;\n}\n\nfunction call_gpt_curl( $messages_array) {\n\n    $headers = [\n        'Content-Type: application/json',\n        'Authorization: Bearer \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0H7cCwDI8'\n    ];\n\n\n    $data = [\n        'model' => 'gpt-3.5-turbo',\n        'messages' => $messages_array,\n        'max_tokens' => 150,\n        'temperature' => 0.5,\n        'top_p' => 0.8,\n    ];\n\n    $ch = curl_init('https://api.openai.com/v1/chat/completions');\n    curl_setopt($ch, CURLOPT_POST, 1);\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\n    curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);\n    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $response_data = json_decode($response, true);\n    $gpt_response = $response_data['choices'][0]['message']['content'];\n\n    return $gpt_response;\n}\n\n\nfunction format_dialogflow_response($gpt_response) {\n\n    $response_data = [\n        \"fulfillmentMessages\" => [\n            [\n                \"text\" => [\n                    \"text\" => [\n                        $gpt_response\n                    ]\n                ]\n            ]\n        ]\n    ];\n\n    return $response_data;\n}\n\nfunction sb_get_conversation($user_id, $conversation_id) {\n    // connect to the database\n    $db = new mysqli(SB_DB_HOST, SB_DB_USER, SB_DB_PASSWORD, SB_DB_NAME);\n\n    // check connection\n    if ($db->connect_error) {\n        die(\"Connection failed: \" . $db->connect_error);\n    }\n\n    // fetch the messages\n    $messages_result = $db->query(\n        \"SELECT sb_messages.message, sb_messages.payload, sb_messages.conversation_id, sb_users.user_type\n        FROM sb_messages \n        INNER JOIN sb_users ON sb_messages.user_id = sb_users.id\n        WHERE sb_messages.conversation_id = $conversation_id \n        ORDER BY sb_messages.id ASC\n        \"\n    );\n\n    if (!$messages_result) {\n        die(\"Query failed: \" . $db->error);\n    }\n\n    $messages = [];\n\n    while ($message = $messages_result->fetch_assoc()) {\n        $messages[] = $message;\n    }\n\n    return [\n        'messages' => $messages,\n    ];\n}\n\n?>\n\n\nHere\u2019s a brief description of our workflow:\n\nThe user sends a message through our web app.\nThe message is transferred to Dialogflow ES, which identifies the user\u2019s intent.\nThe identified intent activates a webhook that fetches the questions along with the available business data.\nThis data is then sent to the OpenAI API.\nOpenAI API processes the question, evaluates the business data, generates a response, and sends it back to Dialogflow ES.\nDialogflow ES then transmits this information back to our web app (support board), where the user can view the response.\n\nThe issue we are encountering is related to the response time from the OpenAI API. Dialogflow ES has a time limit of 5 seconds for the webhook to process the request and return a response. During the development with a free account API (which was the developer\u2019s personal account), we observed the requests were being processed within 3-4 seconds. However, when using an API from a different paid account, the same requests are taking 8-10 seconds to process.\nThis delay poses a significant problem for us as our system is designed to work within the 5 seconds constraint set by Dialogflow ES, and there are no options for us to adjust this timing on our end.\nWe are seeking your assistance to understand why the response times vary so significantly between different accounts and if there is a way to ensure our API requests are processed and responses are returned within the 5-second window. If possible, could we be allocated to a faster processing cluster, or could there be any other solution to reduce this response time?\nLooking forward to your prompt response and assistance in resolving this issue.\nThis is the Error on dialogflow ES:\nWebhook call failed. Error: DEADLINE_EXCEEDED, State: URL_TIMEOUT, Reason: TIMEOUT_WEB.\nBest regards,\nTeam SPOT.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHome \n\n\n\n\nCategories \n\n\n\n\nFAQ/Guidelines \n\n\n\n\nTerms of Service \n\n\n\n\nPrivacy Policy \n\n\n\n\nPowered by Discourse, best viewed with JavaScript enabled\n\n\n\n\nSkip to main content\n\n\n\n\nDocumentationAPI reference\nSign UpLog In\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Community\n        \n\n\n \n\n\n\n\n\n \n\n          Everything\n        \n\n\n \n\n\n\n\n\n\n\nMore\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Resources\n        \n\n\n \n\n\n\n\n\n \n\n          Documentation\n        \n\n\n\n\n\n\n \n\n          API reference\n        \n\n\n\n\n\n\n \n\n          Help center\n        \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n          Categories\n        \n\n\n \n\n\n\n\n\n \n\n          Announcements\n        \n\n\n \n\n\n\n\n\n \n\n          API\n        \n\n\n \n\n\n\n\n\n \n\n          Plugins\n        \n\n\n \n\n\n\n\n\n \n\n          Prompting\n        \n\n\n \n\n\n\n\n\n \n\n          Documentation\n        \n\n\n \n\n\n\n\n\n \n\n          All categories\n        \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n          Tags\n        \n\n\n \n\n\n\n\n\n \n\n          chatgpt\n        \n\n\n \n\n\n\n\n\n \n\n          gpt-4\n        \n\n\n \n\n\n\n\n\n \n\n          codex\n        \n\n\n \n\n\n\n\n\n \n\n          api\n        \n\n\n \n\n\n\n\n\n \n\n          plugin-development\n        \n\n\n \n\n\n\n\n\n \n\n          All tags\n        \n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\u200b\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\u200b\n\n\n\n\nWelcome to the OpenAI Developer Forum!\nWhat to know before posting a new question:\n\nSearch the forum for similar topics - the question might have been discussed before.\nIf the question relates account issues (e.g., billing and login issues), please contact us through our Help Center.\nPlease be kind and helpful in conversations!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n                Urgent Assistance Required - Excessive Response Time with OpenAI API Integration\n              \n\n\n\n API\n\n\napi gpt-35-turbo api-speed \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou have selected 0 posts.\n\n\n\n      select all\n    \n\n\n\n\n\n\n\n\n    cancel selecting\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n          May 16\n        \n\n\n\n\n\n\n\n\n        1 / 1\n      \n\n          May 16\n        \n \n\n\n \n\n\n\n20d ago\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nadmin3420dDear Developers,\nI am writing to request assistance with an issue that we are currently experiencing with the OpenAI API (GPT 3.5 Turbo) integrated into our PHP based web application. Our web app uses Dialogflow ES to power a conversational chatbot, and we have connected Dialogflow ES with OpenAI, adding a knowledge module for enhanced functionality using a webhook.\nHere is the code:\n//ini_set('display_errors', 1);\n//ini_set('display_startup_errors', 1);\n//error_reporting(E_ALL);\n\nheader('Content-Type: application/json');\nrequire('../config.php');\ninclude 'vendor/autoload.php';\nuse Orhanerday\\OpenAi\\OpenAi;\n\n$data = json_decode(file_get_contents('php://input'), true);\n$query = $data['queryResult']['queryText'];\n//$query = \"what is SPOT\";\n\nif($query == \"sb-follow-up-form\"){\n    return;\n}\n\n$url = \"knowledge.txt\";\n$content = file_get_contents($url);\n\n$messages_array = [];\n$messages_array[] = ['role' => 'system', 'content' => 'Following is information about our business, You have to act like you are the a human agent in our support system and do not let the user know about the fact you are an AI language model. This is information of my business:' . $content. 'If the user says or ask anything which is irrelevant with spot, Deny that request or dont respond to that statement by saying that you cannot help them with anything out of SPOT, make sure to not blow your cover by saying you are an AI model. If the user asks anything about you, you are supposed to introduce yourself as \"Agent for SPOT Support\", If they ask you how are you or anything regarding your or their feelings, make sure to never state that you are an AI Lanugage model.'];\n$support_board_data = $data['originalDetectIntentRequest']['payload']['support_board'];\n$conversation_id = $support_board_data['conversation_id'];\n$user_id = $support_board_data['user_id'];\n\n//$conversation_id = 293;\n//$user_id = 334;\n\n$is_last_automated = false;\n\nif($conversation_id){\n    $conversation_data = sb_get_conversation($user_id, $conversation_id);\n    foreach ($conversation_data['messages'] as $message) {\n        $role = ($message['user_type'] === 'agent' || $message['user_type'] === 'bot') ? 'assistant' : 'user';\n        if (strpos($message['message'], '[email id=') === false ) {\n            if (!empty($message['payload'])) {\n                $payload = json_decode($message['payload'], true);\n                if (isset($payload['rich-messages']['sb-follow-up-form']['result'])) {\n                    $result = $payload['rich-messages']['sb-follow-up-form']['result'];\n                    $first_name = $result['first_name'][0] ?? '';\n                    $last_name = $result['last_name'][0] ?? '';\n                    $email = $result['email'][0] ?? '';\n                    $phone = $result['phone'][0] ?? '';\n                    $content = \"Follwing is my information: First Name: $first_name, Last Name: $last_name, Email: $email, Phone: $phone\";\n                    $messages_array[] = ['role' => \"user\", 'content' => $content];\n                    $is_last_automated = true;\n                } else {\n                    $messages_array[] = ['role' => $role, 'content' => $message['message']];\n                    $is_last_automated = false;\n                }\n            }else{\n                $messages_array[] = ['role' => $role, 'content' => $message['message']];\n                $is_last_automated = false;\n            }\n        }\n    }\n}\n$messages_array[] = ['role' => 'user', 'content' => $query];\n\nif($query && $is_last_automated == false)\n{   \n    $open_ai = new OpenAi(\"\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0H7cCwDI8\");\n    $gpt_response = call_gpt($open_ai, $messages_array);\n    //$gpt_response = \"I cant answer that :D\";\n    $dialogflow_response = format_dialogflow_response($gpt_response);\n\n    echo json_encode($dialogflow_response);\n}\n\n\n\nfunction call_gpt($open_ai, $messages_array) {\n\n    \n    $headers = [\n        'Content-Type: application/json',\n        'Authorization: Bearer '\n    ];\n\n\n    $chat = $open_ai->chat([\n        'model' => 'gpt-3.5-turbo',\n        'messages' => $messages_array,\n        'max_tokens' => 150,\n        'temperature' => 0.5,\n        'top_p' => 0.8,\n    ]);\n\n    $response_data = json_decode($chat, true);\n    $gpt_response = $response_data['choices'][0]['message']['content'];\n\n    return $gpt_response;\n}\n\nfunction call_gpt_curl( $messages_array) {\n\n    $headers = [\n        'Content-Type: application/json',\n        'Authorization: Bearer \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0H7cCwDI8'\n    ];\n\n\n    $data = [\n        'model' => 'gpt-3.5-turbo',\n        'messages' => $messages_array,\n        'max_tokens' => 150,\n        'temperature' => 0.5,\n        'top_p' => 0.8,\n    ];\n\n    $ch = curl_init('https://api.openai.com/v1/chat/completions');\n    curl_setopt($ch, CURLOPT_POST, 1);\n    curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);\n    curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);\n    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));\n\n    $response = curl_exec($ch);\n    curl_close($ch);\n\n    $response_data = json_decode($response, true);\n    $gpt_response = $response_data['choices'][0]['message']['content'];\n\n    return $gpt_response;\n}\n\n\nfunction format_dialogflow_response($gpt_response) {\n\n    $response_data = [\n        \"fulfillmentMessages\" => [\n            [\n                \"text\" => [\n                    \"text\" => [\n                        $gpt_response\n                    ]\n                ]\n            ]\n        ]\n    ];\n\n    return $response_data;\n}\n\nfunction sb_get_conversation($user_id, $conversation_id) {\n    // connect to the database\n    $db = new mysqli(SB_DB_HOST, SB_DB_USER, SB_DB_PASSWORD, SB_DB_NAME);\n\n    // check connection\n    if ($db->connect_error) {\n        die(\"Connection failed: \" . $db->connect_error);\n    }\n\n    // fetch the messages\n    $messages_result = $db->query(\n        \"SELECT sb_messages.message, sb_messages.payload, sb_messages.conversation_id, sb_users.user_type\n        FROM sb_messages \n        INNER JOIN sb_users ON sb_messages.user_id = sb_users.id\n        WHERE sb_messages.conversation_id = $conversation_id \n        ORDER BY sb_messages.id ASC\n        \"\n    );\n\n    if (!$messages_result) {\n        die(\"Query failed: \" . $db->error);\n    }\n\n    $messages = [];\n\n    while ($message = $messages_result->fetch_assoc()) {\n        $messages[] = $message;\n    }\n\n    return [\n        'messages' => $messages,\n    ];\n}\n\n?>\n\n\nHere\u2019s a brief description of our workflow:\n\nThe user sends a message through our web app.\nThe message is transferred to Dialogflow ES, which identifies the user\u2019s intent.\nThe identified intent activates a webhook that fetches the questions along with the available business data.\nThis data is then sent to the OpenAI API.\nOpenAI API processes the question, evaluates the business data, generates a response, and sends it back to Dialogflow ES.\nDialogflow ES then transmits this information back to our web app (support board), where the user can view the response.\n\nThe issue we are encountering is related to the response time from the OpenAI API. Dialogflow ES has a time limit of 5 seconds for the webhook to process the request and return a response. During the development with a free account API (which was the developer\u2019s personal account), we observed the requests were being processed within 3-4 seconds. However, when using an API from a different paid account, the same requests are taking 8-10 seconds to process.\nThis delay poses a significant problem for us as our system is designed to work within the 5 seconds constraint set by Dialogflow ES, and there are no options for us to adjust this timing on our end.\nWe are seeking your assistance to understand why the response times vary so significantly between different accounts and if there is a way to ensure our API requests are processed and responses are returned within the 5-second window. If possible, could we be allocated to a faster processing cluster, or could there be any other solution to reduce this response time?\nLooking forward to your prompt response and assistance in resolving this issue.\nThis is the Error on dialogflow ES:\nWebhook call failed. Error: DEADLINE_EXCEEDED, State: URL_TIMEOUT, Reason: TIMEOUT_WEB.\nBest regards,\nTeam SPOT.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReply\n\n\n\n\n\n\n    Related Topics\n  \n\n\nTopic\nReplies\nViews\nActivity\n\n\n\n\n\n\nCurl api.openai.com is slow, 15-30s/req\n\n\nAPI\n\n\n\n\n22\n\n\n\n1.8k\n\n\nMar 15\n\n\n\n\n\n\nTake large response time to process arabic text in gpt-3.5-turbo model\n\n\nChatGPT\n\n\n\n\n0\n\n\n\n121\n\n\nMar 24\n\n\n\n\n\n\n\n\nOpen AI error response in PHP\n\n\nAPI\n\n\n\n\n6\n\n\n\n284\n\n\n27d\n\n\n\n\n\n\nHow can I improve response times from the OpenAI API while generating responses based on our knowledge base?\n\n\nAPI\nchatgpt api \n\n\n\n\n2\n\n\n\n171\n\n\n6d\n\n\n\n\n\n\nDiscrepancy in Response Speed between GPT-3.5-turbo API and ChatGPT UI\n\n\nAPI\nchatgpt api gpt-35-turbo \n\n\n\n\n3\n\n\n\n611\n\n\n12d\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\u200b\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n          OpenAI \u00a9 2015\u200a\u2013\u200a2023\n        \n\n          The OpenAI developer forum is a place to connect with other people building with OpenAI models.\n        \n\n\n\n\n\n                Research\n              \n\n\n\n                      Overview\n                    \n\n\n\n                      Index\n                    \n\n\n\n\n\n                Product\n              \n\n\n\n                      Overview\n                    \n\n\n\n                      Customer stories\n                    \n\n\n\n                      Safety standards\n                    \n\n\n\n                      Pricing\n                    \n\n\n\n\n\n                Safety\n              \n\n\n\n                      Overview\n                    \n\n\n\n                      Security\n                    \n\n\n\n\n\n                Help\n              \n\n\n\n                      Support\n                    \n\n\n\n\n\n                Developers\n              \n\n\n\n                      Documentation\n                    \n\n\n\n                      Service status\n                    \n\n\n\n                      Examples\n                    \n\n\n\n\n\n                Company\n              \n\n\n\n                      About\n                    \n\n\n\n                      Blog\n                    \n\n\n\n                      Careers\n                    \n\n\n\n                      Charter\n                    \n\n\n\n\n\n\n\n\n              Terms & policies\n            \n\n              Privacy policy\n            \n\n              Brand guidelines\n            \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u200b\n\n\n\n \n\n\n \n\n\n\n\n\n\n\n\n\n  Invalid date\n\n\n\n\n  Invalid date\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"

result = tokenizer.tokenize(text)

print("Text size:", len(text))

count = 0

for res in result:
    count += len(res)

print(len(result[0]))
print(len(result[1]))

print("Token size:", count)

tokenizer2 = GPT2Tokenizer.from_pretrained("gpt2")
token_cnt = len(tokenizer2.encode(text))
max_tokens = 4000


print("Tokens:", token_cnt," of ", max_tokens, "\n ratio:", math.ceil( token_cnt/max_tokens))



part = text[:math.ceil(len(text)*max_tokens/token_cnt)]
token_cnt = len(tokenizer2.encode(part))
print("Tokens:", token_cnt," of ", max_tokens, "\n ratio:", math.ceil( token_cnt/max_tokens))

step = 1000
def divide(text_part, tok, step = 1000, m_tokens = 4000):
    len_part = 0
    i_part = ""
    index = 0
    while index < 1000 and step*index < len(text_part):
        prev_part = i_part
        if (step*(index + 1)) > len(text_part):
            i_part += text_part[step*index :]
        else:
            i_part += text_part[step*index : (step*(index + 1) )]
        index += 1
        len_part = len(tok.encode(i_part))
        if len_part > m_tokens:
            return prev_part
    return i_part
        
part = divide(text, tokenizer2)

token_cnt = len(tokenizer2.encode(part))
print("Tokens:", token_cnt," of ", max_tokens, "\n ratio:", math.ceil( token_cnt/max_tokens))

def getParts(in_text, tok):
    i = 0
    parts = []
    while i < 10:
        part = divide(in_text, tok)
        if part == "":
            break
        in_text = in_text[len(part) : ]
        parts.append( part)
        i += 1
    return parts

n_parts = getParts(text, tokenizer2)
text_parts = ""
for p in n_parts:
    text_parts += p

print("Same", text_parts == text)
     

